{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8f33f6",
   "metadata": {},
   "source": [
    "Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb11f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stopwords_fr = set(stopwords.words('french'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a883fa",
   "metadata": {},
   "source": [
    "Download Word2Vec model.\n",
    "Here the link to download it http://vectors.nlpl.eu/repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ee82f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('Path to the model on your computer', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c1d394",
   "metadata": {},
   "source": [
    "Upload train and test sets.The generation process we explained in dataset_generation file https://github.com/preste-ai/rnd-nlp-cot-chatgptdatagen/blob/main/dataset_generation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27860a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('Your train set', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "772824ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('Your test set', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c749adbc",
   "metadata": {},
   "source": [
    "Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "638516b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of contractions in French language\n",
    "contractions ={\n",
    "    \"l'\" : \"le \",\n",
    "    \"d'\" : \"de \",\n",
    "    \"j'\" : \"je \",\n",
    "    \"m'\" : \"me \",\n",
    "    \"qu'\" : \"que \",\n",
    "    \"n'\" : \"ne \",\n",
    "    \"s'\" : \"se \"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4313f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(column):\n",
    "    # Lowercase\n",
    "    column = column.str.lower()\n",
    "    # remove contractions\n",
    "    def remove_contractions(column):\n",
    "        pattern = re.compile(r\"\\b(\" + '|'.join(contractions.keys()) + r\")\\b\")\n",
    "        return pattern.sub(lambda match: contractions[match.group(0)], column)\n",
    "    column = column.apply(remove_contractions)\n",
    "    # Tokenization\n",
    "    column = column.apply(word_tokenize)\n",
    "    # Punctuation removal\n",
    "    column = column.apply(lambda tokens: [token for token in tokens if token not in string.punctuation])\n",
    "    # stopword removal\n",
    "    column = column.apply(lambda tokens: [token for token in tokens if token not in stopwords_fr])\n",
    "    # Lemmatization\n",
    "    column = column.apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "    return column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c3698",
   "metadata": {},
   "source": [
    "Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fae955c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_embeddigs(features):\n",
    "    documents = []\n",
    "    skipped_sent = []\n",
    "    for feature in features:\n",
    "        word_vectors = []\n",
    "        for i, word in enumerate(feature):\n",
    "            \n",
    "            try: \n",
    "                word_vectors.append(np.resize(model[word], (1,100)))\n",
    "            except KeyError:\n",
    "                #print(word)\n",
    "                pass\n",
    "        try:\n",
    "            if len(word_vectors) != 0:\n",
    "                documents.append(np.mean(word_vectors, axis=0))\n",
    "            else:\n",
    "                skipped_sent.append(i)\n",
    "        except ValueError:\n",
    "            skipped_sent.append(i)\n",
    "        \n",
    "    documents = np.concatenate(documents)\n",
    "    features_embeddings = np.squeeze(documents)\n",
    "    return features_embeddings, skipped_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfda185",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b22c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(train_set, test_set):\n",
    "    train_set[\"Features\"] = train_set[\"Phrases\"] + \" \" + train_set[\"Rationals\"]\n",
    "    train_set['clean_features'] = preprocess_text(train_set['Features'])\n",
    "    test_set['clean_features'] = preprocess_text(test_set['text-fr'])\n",
    "    labels = train_set.loc[:, \"labels_code\"].values\n",
    "    labels_for_test = test_set.loc[:, \"labels\"].values\n",
    "    features = train_set['clean_features'].tolist()\n",
    "    features_for_test = test_set['clean_features'].tolist()\n",
    "    processed_features, skipped_sent_idx = creating_embeddigs(features)\n",
    "    new_labels = np.delete(labels, skipped_sent_idx)\n",
    "    processed_features_for_test, skipped_sent_idx_for_test = creating_embeddigs(features_for_test)\n",
    "    new_labels_for_test = np.delete(labels_for_test, skipped_sent_idx_for_test)\n",
    "    X_train = processed_features\n",
    "    y_train = new_labels\n",
    "    X_test = processed_features_for_test\n",
    "    y_test = new_labels_for_test\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "070a298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(train_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9eec8e",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63eb75a",
   "metadata": {},
   "source": [
    "Function to get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97722453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, y_pred, y_test): \n",
    "    test_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return test_report, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af5cc0f",
   "metadata": {},
   "source": [
    "Train the model and get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64f96179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(X_train, y_train, X_test, y_test):\n",
    "    # Linear SVC:\n",
    "    text_clf_lsvc = Pipeline([('clf', LinearSVC())])\n",
    "    text_clf_lsvc.fit(X_train, y_train)\n",
    "    # Form a prediction set\n",
    "    prediction_linear_SVC = text_clf_lsvc.predict(X_test)\n",
    "    test_report, accuracy, precision, recall, f1 = get_metrics(model, prediction_linear_SVC, y_test)\n",
    "    # Saving model to pickle file\n",
    "    with open(\"SVM_with_rationals.pkl\", \"wb\") as file: # file is a variable for storing the newly created file, it can be anything.\n",
    "        pickle.dump(text_clf_lsvc, file) # Dump function is used to write the object into the created file in byte format.\n",
    "    return test_report, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53c23935",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_report, accuracy, precision, recall, f1 = run_svm(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56d10e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8841870824053452\n",
      "Precision:  0.8963142417527906\n",
      "Recall:  0.8841870824053452\n",
      "F1:  0.8852016705254259\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e997a9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
